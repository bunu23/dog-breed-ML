{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16859,"status":"ok","timestamp":1716418871930,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"usyIriq41iBi","outputId":"7195b397-b86c-4113-b633-253d05008d50"},"outputs":[],"source":["import torch\n","import torchvision.models as models\n","\n","\n","# define VGG16 model\n","VGG16 = models.vgg16(pretrained=True)\n","\n","# check if CUDA is available\n","use_cuda = torch.cuda.is_available()\n","\n","# move model to GPU if CUDA is available\n","if use_cuda:\n","    VGG16 = VGG16.cuda()"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1716419760802,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"121PQG7O1mhT"},"outputs":[],"source":["# Making Predictions with a Pre-trained Model\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","def VGG16_predict(img_path):\n","   \n","\n","    my_image = Image.open(img_path).convert('RGB')\n","\n","    transform = transforms.Compose([\n","                        transforms.Resize(size=(244,244)),  #transforms.Resize() to resize the image to the required dimensions.\n","                        transforms.ToTensor()])\n","\n","    my_image = transform(my_image)[:3,:,:].unsqueeze(0)\n","\n","\n","    VGG16.cpu()\n","\n","    result = VGG16(my_image)\n","\n","    return torch.max(result,1)[1].item() # predicted class index"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2438,"status":"ok","timestamp":1716419764698,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"YWNQeeB32A5d","outputId":"cc58048a-514d-41c4-bee4-215a304dec33"},"outputs":[{"data":{"text/plain":["162"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["VGG16_predict(dog_files[1])"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":135,"status":"ok","timestamp":1716419765641,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"WXtGSyeL2DOW"},"outputs":[],"source":["def dog_detector(img_path):\n","  \n","    return VGG16_predict(img_path) in range(151, 269)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1244,"status":"ok","timestamp":1716419770225,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"JcwlsH4N2Ncw","outputId":"a201cf61-98d6-4635-cd11-63d19dabb061"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["dog_detector(dog_files[800])"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":151,"status":"ok","timestamp":1716419774204,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"QqirG3Yh2O_p"},"outputs":[],"source":["import os\n","from torchvision import datasets\n","\n","### data loaders for training, validation, and test sets\n","\n","# It initializes parameters for data loading, such as the number of subprocesses and batch size.\n","# Additionally, it defines paths to the training, validation, and test datasets.\n","# The paths provided here assume a directory structure where images are organized into train, test, and validation folders.\n","# Finally, it applies normalization to the images .\n","# number of subprocesses to use for data loading.Setting it to 0 means that the data will be loaded in the main process.\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 20\n","\n","\n","train_path = 'dogImages/train'\n","valid_path = 'dogImages/valid'\n","test_path= 'dogImages/test'\n","\n","# normalizing all images as per imagenet standards\n","standard_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                          std=[0.229, 0.224, 0.225])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":132,"status":"ok","timestamp":1716419777008,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"tgSsOx5I2zGI"},"outputs":[],"source":["# adding a set of transformations to be performed in train/test/valid images, doing some data augmentations for better training\n","# This dictionary, data_transforms_dict, contains sets of image transformations\n","# to be applied to the training, validation, and test datasets. These transformations\n","# are crucial for preprocessing the input data and enhancing the training process\n","# of deep learning models.\n","\n","# For the 'train' set, data augmentation techniques like random resized crop and\n","# horizontal flip are employed to introduce variability and prevent overfitting.\n","# Additionally, the images are converted to PyTorch tensors and normalized based\n","# on the ImageNet standards.\n","\n","# The 'valid' set applies resizing and center cropping to ensure uniformity in\n","# input dimensions for validation data. Similar to the 'train' set, images are\n","# converted to tensors and normalized.\n","\n","# The 'test' set prepares the test data by resizing images to a fixed size,\n","# converting them to tensors, and applying normalization.\n","\n","\n","data_transforms_dict = {'train': transforms.Compose([transforms.RandomResizedCrop(224),\n","                                     transforms.RandomHorizontalFlip(),\n","                                     transforms.ToTensor(),\n","                                     standard_normalize]),\n","\n","                   'valid': transforms.Compose([transforms.Resize(256),\n","                                     transforms.CenterCrop(224),\n","                                     transforms.ToTensor(),\n","                                     standard_normalize]),\n","\n","                   'test': transforms.Compose([transforms.Resize(size=(224,224)),\n","                                     transforms.ToTensor(),\n","                                     standard_normalize])\n","                  }"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1098,"status":"ok","timestamp":1716419780785,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"tLioISN03RnD"},"outputs":[],"source":["# now applying data transformations to each images in respective folders\n","# This code block prepares dataset objects for the training, validation, and test datasets\n","# by applying the specified data transformations to each image in their respective folders.\n","# It utilizes the datasets.ImageFolder class from torchvision datasets to create dataset objects\n","# for each dataset, specifying the paths to the image folders and the corresponding transformations\n","# to be applied during data loading.\n","\n","train_data = datasets.ImageFolder(train_path, transform = data_transforms_dict['train'])\n","valid_data = datasets.ImageFolder(valid_path, transform = data_transforms_dict['valid'])\n","test_data = datasets.ImageFolder(test_path, transform = data_transforms_dict['test'])"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716419780935,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"wcCBTHll3TfD"},"outputs":[],"source":["# This code block sets up data loaders for the training, validation, and test datasets,\n","# utilizing the torch.utils.data.DataLoader class. Data loaders are essential for\n","# efficiently loading and iterating through batches of data during the training and\n","# evaluation of deep learning models.\n","\n","# For each dataset (train, validation, test), a data loader is created with the\n","# corresponding dataset object, batch size, number of workers for data loading, and\n","# shuffling option.\n","\n","# The resulting data loaders are organized into a dictionary called loaders_scratch,\n","# with keys 'train', 'valid', and 'test', each corresponding to their respective data loaders.\n","\n","\n","train_loader = torch.utils.data.DataLoader(train_data,\n","                                           batch_size=batch_size,\n","                                           num_workers=num_workers,\n","                                           shuffle=True)\n","\n","valid_loader = torch.utils.data.DataLoader(valid_data,\n","                                           batch_size=batch_size,\n","                                           num_workers=num_workers,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(test_data,\n","                                           batch_size=batch_size,\n","                                           num_workers=num_workers,\n","                                           shuffle=True)\n","\n","loaders_scratch = {'train': train_loader,\n","                'valid': valid_loader,\n","                'test': test_loader}"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128,"status":"ok","timestamp":1716419783825,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"HJ6PPapW3WH6","outputId":"769b9db1-aaa0-4356-bb60-013a5837f63d"},"outputs":[{"data":{"text/plain":["(6680, 835, 836)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# This code block retrieves the number of samples in the training, validation, and test datasets.\n","# It provides insight into the size of each dataset, which is crucial for understanding\n","# the distribution of data across different sets and for monitoring the training process\n","# and evaluating the performance of the model.\n","\n","len(train_data), len(valid_data), len(test_data)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":117,"status":"ok","timestamp":1716419785135,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"4q4BkJTM6uFZ"},"outputs":[],"source":["# This code block creates a copy of the previously defined data loaders\n","# stored in the loaders_scratch dictionary. These loaders were set up\n","# for the training, validation, and test datasets and are ready for use\n","# in training and evaluating deep learning models.\n","loaders_transfer = loaders_scratch.copy()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2470,"status":"ok","timestamp":1716419789590,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"l8AwEmkA3Z_z","outputId":"8902cc87-c84b-456d-a215-3fd50ac6dfdd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 87.7MB/s]\n"]},{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["import torchvision.models as models\n","import torch.nn as nn\n","\n","# This code block imports the ResNet-50 architecture from the torchvision.models module\n","# and instantiates a pre-trained ResNet-50 model using the models.resnet50() function.\n","#The pre-trained model has been trained\n","# on the ImageNet dataset, which allows it to extract meaningful features from images.\n","# This pre-trained ResNet-50 model will serve as the base architecture for transfer learning.\n","\n","model_transfer = models.resnet50(pretrained=True)\n","model_transfer"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131,"status":"ok","timestamp":1716419792349,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"cd60UXD93-du","outputId":"f35005ee-e52e-4404-88a9-bf33491f5b95"},"outputs":[{"data":{"text/plain":["Linear(in_features=2048, out_features=1000, bias=True)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["#The model_transfer.fc attribute represents the final fully connected layer of the ResNet-50 model. This layer is responsible for taking the features extracted by the convolutional layers of the ResNet-50 backbone and transforming them into the final output of the model.\n","model_transfer.fc"]},{"cell_type":"markdown","metadata":{"id":"CGI5SWp4g0jc"},"source":["in_features: The number of input features to the fully connected layer, which is 2048 in this case. This typically corresponds to the number of output features from the preceding layers, often the output of the last convolutional layer in the neural network architecture.\n","\n","out_features: The number of output features or classes produced by the fully connected layer, which is 1000 in this case. This indicates that the output of this layer is a vector of size 1000, where each element represents the model's confidence score for a specific class.\n","\n","bias: A boolean indicating whether the layer includes a bias term in the linear transformation. When bias=True, as in this case, the layer includes a bias term.\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1716419798312,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"K-9sTVzX4B1N","outputId":"1c98287c-de40-4183-a73d-3325c5bfdc3f"},"outputs":[{"data":{"text/plain":["(2048, 1000)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","model_transfer.fc.in_features and model_transfer.fc.out_features retrieves the number of input features and output features of the fully connected layer (fc) in the model_transfer.\n"," This is helpful for understanding the dimensions of the data flowing through this layer in the neural network.\n","\n","\n","\n","model_transfer.fc.in_features: This attribute returns the number of input features to the fully connected layer.\n","\n","model_transfer.fc.out_features: This attribute returns the number of output features or classes produced by the fully connected layer.\n","\n","\n","\"\"\"\n","model_transfer.fc.in_features, model_transfer.fc.out_features"]},{"cell_type":"markdown","metadata":{"id":"NaaCP1bYhdm2"},"source":[" represents the dimensions of the fully connected layer (fc) in the model_transfer. Specifically:\n","\n","2048 corresponds to the number of input features to the fully connected layer. This indicates that the fully connected layer receives 2048-dimensional input.\n","\n","1000 corresponds to the number of output features or classes produced by the fully connected layer. This indicates that the fully connected layer produces a 1000-dimensional output, with each dimension representing the confidence score for one of the 1000 ImageNet classes."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1716419801550,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"6GcifW-14Dm2","outputId":"b779d469-0544-4b62-ed19-15cd5123fac5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Linear(in_features=2048, out_features=133, bias=True)\n"]}],"source":["\"\"\"\n","prepares a pre-trained ResNet-50 model for transfer learning on a\n","new task of classifying dog breeds by adapting the final classification\n","layer and freezing the weights of the pre-trained layers to preserve their learned feature\n","\"\"\"\n","total_dog_classes = 133 #number of dog breeds that the model will be trained to classify.\n","\n","\n","\n","# Freeze training for all \"features\" layers\n","for param in model_transfer.parameters():\n","    param.requires_grad = False\n","\n","\n","num_in_features = model_transfer.fc.in_features\n","\n","#retrieving the final fully connected layer of resnet50 and replace it with our own linear layer\n","\n","model_transfer.fc = nn.Linear(num_in_features, total_dog_classes)  # total_dog_classes is 133 as defined above\n","\n","if use_cuda:\n","    model_transfer = model_transfer.cuda()\n","\n","print(model_transfer.fc)"]},{"cell_type":"markdown","metadata":{"id":"PSco64fRjW_Y"},"source":[]},{"cell_type":"markdown","metadata":{"id":"qv9dIzsUjH4h"},"source":[]},{"cell_type":"markdown","metadata":{"id":"w4cyUbSZiJ92"},"source":["output:\n","represents the structure of the modified final fully connected layer (fc) after the code execution.\n","\n","in_features=2048: Indicates that the modified fully connected layer has 2048 input features. This corresponds to the number of output features from the preceding layers, typically the output of the last convolutional layer in the ResNet-50 architecture.\n","\n","out_features=133: Indicates that the modified fully connected layer produces 133 output features or classes. This is aligned with the specific task of classifying dog breeds, where there are 133 distinct classes representing different dog breeds.\n","\n","bias=True: Indicates that the modified fully connected layer includes a bias term in the linear transformation. When bias=True, a bias term is added to each output feature.\n","\n","output Linear(in_features=2048, out_features=133, bias=True) signifies that the final fully connected layer (fc) of the ResNet-50 model has been successfully modified to suit the new task of classifying dog breeds. It has 2048 input features and produces 133 output features, each representing the confidence score for one of the 133 dog breeds."]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":228,"status":"ok","timestamp":1716419809576,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"R8_mNSDq5OT8"},"outputs":[],"source":["#Specify Loss Function and Optimizer\n","\n","import torch.optim as optim\n","\n","criterion_transfer = nn.CrossEntropyLoss()\n","optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr=0.005)"]},{"cell_type":"markdown","metadata":{"id":"mhjQdITwjttw"},"source":["Specifies a loss function and an optimizer for training the modified ResNet-50 model (model_transfer) on the task of classifying dog breeds. Here's what each part of the code does:\n","\n","criterion_transfer = nn.CrossEntropyLoss(): This line defines the loss function used for the classification task. nn.CrossEntropyLoss() is a common choice for multi-class classification problems. It combines the softmax function and the negative log-likelihood loss, making it suitable for models that output probability distributions over multiple classes. In this case, the model will predict the probability distribution over the 133 dog breed classes, and CrossEntropyLoss will measure the difference between the predicted distribution and the true labels.\n","\n","optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr=0.005): This line defines the optimizer used to update the weights of the model during training. optim.Adam is a popular optimization algorithm known for its adaptive learning rate method. It adjusts the learning rates for each parameter individually, which can lead to faster convergence and better performance. model_transfer.fc.parameters() specifies that only the parameters of the final fully connected layer (fc) should be optimized, as the rest of the model's parameters are frozen. The learning rate (lr) is set to 0.005, which determines the step size taken during optimization."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1716419812021,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"fBXVFY1r6eH7"},"outputs":[],"source":["#Train and Validate the Model\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n","    \"\"\"returns trained model\"\"\"\n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = np.Inf\n","\n","    for epoch in range(1, n_epochs+1):\n","        # initialize variables to monitor training and validation loss\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(loaders['train']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            ## find the loss and update the model parameters accordingly\n","            ## record the average training loss, using something like\n","            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","\n","            # clear the gradient of previous iterations\n","            optimizer.zero_grad()\n","\n","            output = model(data)\n","\n","            # calculate the loss by comparing the generated output with actual label\n","            loss = criterion(output, target)\n","\n","            # do backpropagation step\n","            loss.backward()\n","\n","            # updating weights during backprop\n","            optimizer.step()\n","\n","            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","\n","            if batch_idx % 100 == 0:\n","                print('Epoch %d, Batch %d loss: %.6f' %(epoch, batch_idx + 1, train_loss))\n","\n","\n","\n","        ######################\n","        # validate the model #\n","        ######################\n","        model.eval()\n","        for batch_idx, (data, target) in enumerate(loaders['valid']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            ## update the average validation loss\n","\n","            output = model(data)\n","\n","            loss = criterion(output, target)\n","\n","            valid_loss = valid_loss + ((1/(batch_idx+1)) * (loss.data - valid_loss))\n","\n","\n","        # print training/validation statistics\n","        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch,\n","            train_loss,\n","            valid_loss\n","            ))\n","\n","        ## TODO: save the model if validation loss has decreased\n","        if valid_loss < valid_loss_min:\n","\n","            torch.save(model.state_dict(), save_path)\n","\n","            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n","\n","            # updating the minimum validation loss\n","            valid_loss_min = valid_loss\n","\n","    # return trained model\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"lymlo5CfkJPt"},"source":["This training function is designed to train a neural network model using a specified number of epochs and provided data loaders for both training and validation sets.\n","\n","function orchestrates the training and validation process of a neural network model, monitoring the loss values and saving the model's state when a lower validation loss is achieved, thereby facilitating model checkpointing and saving the best-performing model.\n","\n","1.Initialization:\n","   - The function initializes a variable `valid_loss_min` to positive infinity. This variable will be used to track the minimum validation loss observed during training.\n","\n","2. Training Loop (for each epoch):\n","   - The function iterates over each epoch from 1 to `n_epochs`.\n","   - Within each epoch, it initializes variables `train_loss` and `valid_loss` to track the training and validation losses, respectively.\n","\n","3. Training Phase:\n","   - The model is set to training mode (`model.train()`).\n","   - For each batch in the training data loader (`loaders['train']`), the function performs the following steps:\n","     - Moves the data and target labels to the GPU if CUDA is available (`use_cuda`).\n","     - Clears the gradients of the optimizer (`optimizer.zero_grad()`).\n","     - Forward passes the input data through the model to obtain the output predictions.\n","     - Calculates the loss between the model predictions and the target labels.\n","     - Performs backpropagation to compute gradients.\n","     - Updates the model parameters using the optimizer (`optimizer.step()`).\n","     - Computes and updates the average training loss (`train_loss`).\n","\n","4. Validation Phase:\n","   - The model is set to evaluation mode (`model.eval()`).\n","   - For each batch in the validation data loader (`loaders['valid']`), the function performs the following steps:\n","     - Moves the data and target labels to the GPU if CUDA is available (`use_cuda`).\n","     - Forward passes the input data through the model to obtain the output predictions.\n","     - Calculates the validation loss between the model predictions and the target labels.\n","     - Computes and updates the average validation loss (`valid_loss`).\n","\n","5. Logging and Saving:\n","   - After processing all batches in an epoch, the function prints the training and validation losses for that epoch.\n","   - If the current validation loss is lower than the minimum validation loss observed so far (`valid_loss_min`), the function saves the model's state dictionary to the specified `save_path`.\n","   - It then updates `valid_loss_min` to the current validation loss.\n","\n","6. Return:\n","   - After completing all epochs, the function returns the trained model.\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15012146,"status":"ok","timestamp":1716434827624,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"GYjvdyvB5oGf","outputId":"81c50c52-c116-4fc1-eeb9-459a727a9156"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Batch 1 loss: 4.804659\n","Epoch 1, Batch 101 loss: 5.512602\n","Epoch 1, Batch 201 loss: 4.366426\n","Epoch 1, Batch 301 loss: 3.948845\n","Epoch: 1 \tTraining Loss: 3.838014 \tValidation Loss: 1.887553\n","Validation loss decreased (inf --> 1.887553).  Saving model ...\n","Epoch 2, Batch 1 loss: 3.387529\n","Epoch 2, Batch 101 loss: 2.713449\n","Epoch 2, Batch 201 loss: 2.841927\n","Epoch 2, Batch 301 loss: 2.757945\n","Epoch: 2 \tTraining Loss: 2.735197 \tValidation Loss: 2.141465\n","Epoch 3, Batch 1 loss: 5.353657\n","Epoch 3, Batch 101 loss: 2.690471\n","Epoch 3, Batch 201 loss: 2.721024\n","Epoch 3, Batch 301 loss: 2.796488\n","Epoch: 3 \tTraining Loss: 2.850372 \tValidation Loss: 1.908857\n","Epoch 4, Batch 1 loss: 2.021873\n","Epoch 4, Batch 101 loss: 2.593591\n","Epoch 4, Batch 201 loss: 2.646577\n","Epoch 4, Batch 301 loss: 2.721172\n","Epoch: 4 \tTraining Loss: 2.727392 \tValidation Loss: 1.759367\n","Validation loss decreased (1.887553 --> 1.759367).  Saving model ...\n","Epoch 5, Batch 1 loss: 4.434908\n","Epoch 5, Batch 101 loss: 2.493634\n","Epoch 5, Batch 201 loss: 2.555387\n","Epoch 5, Batch 301 loss: 2.679195\n","Epoch: 5 \tTraining Loss: 2.723645 \tValidation Loss: 1.857348\n"]}],"source":["# train the model\n","\n","n_epochs = 5\n","\n","model_transfer =  train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer,\n","                        criterion_transfer, use_cuda, 'model_transfer.pt')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QXoIPRc7kgyq"},"source":["trains the model_transfer neural network model using the train function defined earlier. Here's a breakdown of the parameters and what each part of the code does:\n","\n","n_epochs = 1: Specifies the number of epochs for training. In this case, the model will be trained for one epoch.\n","\n","loaders_transfer: Provides the data loaders for training and validation sets. These loaders contain the training and validation data in batches.\n","\n","model_transfer: Represents the neural network model (pre-trained ResNet-50) that will be trained on the task of classifying dog breeds. This model has already been modified for the new task.\n","\n","optimizer_transfer: Specifies the optimizer (Adam) used for updating the model's parameters during training.\n","\n","criterion_transfer: Defines the loss function (CrossEntropyLoss) used to compute the training loss during training.\n","\n","use_cuda: Indicates whether CUDA (GPU) should be used for training. If True, the model and data will be moved to the GPU.\n","\n","'model_transfer.pt': Specifies the file path where the trained model's state dictionary will be saved after training. This allows for model checkpointing and saving the best-performing model."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1211,"status":"ok","timestamp":1716434883855,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"z-O6w8146cp2","outputId":"a26e4bce-f0d6-43f5-fd97-bc154921e44b"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["#loads the trained model that achieved the best validation accuracy during training\n","model_transfer.load_state_dict(torch.load('model_transfer.pt'))"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716434885450,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"i7ckDy6lJKHg"},"outputs":[],"source":["#test the model\n","\n","def test(loaders, model, criterion, use_cuda):\n","\n","    # monitor test loss and accuracy\n","    test_loss = 0.\n","    correct = 0.\n","    total = 0.\n","\n","    model.eval()\n","    for batch_idx, (data, target) in enumerate(loaders['test']):\n","        # move to GPU\n","        if use_cuda:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the loss\n","        loss = criterion(output, target)\n","        # update average test loss\n","        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n","        # convert output probabilities to predicted class\n","        pred = output.data.max(1, keepdim=True)[1]\n","        # compare predictions to true label\n","        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n","        total += data.size(0)\n","\n","    print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (100. * correct / total, correct, total))"]},{"cell_type":"markdown","metadata":{"id":"HfkjoG91l7pE"},"source":["test function to evaluate the performance of a trained neural network model on a test dataset. Here's an explanation of each part of the code:\n","\n","Initialization:\n","\n","Three variables test_loss, correct, and total are initialized to monitor the test loss, the number of correct predictions, and the total number of predictions, respectively.\n","Evaluation Loop:\n","\n","The model is switched to evaluation mode using model.eval(). This disables certain operations like dropout during evaluation.\n","The function iterates over each batch in the test data loader (loaders['test']) using enumerate to get both the batch index and data/target pairs.\n","For each batch, the following steps are performed:\n","If use_cuda is True, the data and target tensors are moved to the GPU using .cuda().\n","The input data is passed through the model to obtain the output predictions (output).\n","The test loss for the batch is computed using the provided loss function (criterion).\n","The average test loss is updated using an online averaging formula.\n","The output probabilities are converted to predicted class labels (pred) using torch.max.\n","The number of correct predictions (correct) is updated by comparing predicted labels to true labels.\n","The total number of predictions (total) is updated by adding the batch size (data.size(0)).\n","Print Results:\n","After evaluating all batches, the function prints the average test loss and the test accuracy.\n","Test accuracy is computed as the percentage of correct predictions out of the total number of predictions."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":616148,"status":"ok","timestamp":1716435507240,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"2xIL1KWHI9sd","outputId":"8a8778d3-9426-4dd7-ffeb-d2c73bb13492"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 1.793391\n","\n","\n","Test Accuracy: 73% (616/836)\n"]}],"source":["test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94,"status":"ok","timestamp":1716410597925,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"0CzG7yt_JCy-","outputId":"5e84ed32-aacd-4069-f5d2-a0cc653b9430"},"outputs":[{"name":"stdout","output_type":"stream","text":["001.Affenpinscher\n","133.Yorkshire_terrier\n"]}],"source":["#Predict Dog Breed with the Model\n","# retrieve and display the first and last classes from the dataset used in the training loader loaders_transfer['train'].\n","\n","\n","print(loaders_transfer['train'].dataset.classes[0])\n","print(loaders_transfer['train'].dataset.classes[-1])"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716410604771,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"mUUdewacM7aW"},"outputs":[],"source":["\n","\"\"\"\n","function takes an image file, processes it, and predicts\n","the breed of the dog in the image using a pre-trained neural network model. I\n","\"\"\"\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","def predict_breed_transfer(img_path, model, class_names):\n","    # load the image and return the predicted breed\n","\n","    # referenced from VGG16_predict() method defined above for processing image accordingly\n","    my_image = Image.open(img_path).convert('RGB')\n","\n","    transform = transforms.Compose([\n","                        transforms.Resize(size=(224,224)),  #resize to (244,244) as per vgg design specification\n","                        transforms.ToTensor(),\n","                        standard_normalize])\n","\n","    my_image = transform(my_image)[:3,:,:].unsqueeze(0)\n","\n","    model = model.cpu()\n","\n","    model.eval()\n","\n","    breed_index = torch.argmax(model(my_image))\n","\n","    return class_names[breed_index]"]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":89,"status":"ok","timestamp":1716410893274,"user":{"displayName":"Bunu Bhattarai","userId":"18368468491242336764"},"user_tz":240},"id":"vQWOCe6ZNaMq"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","\n","def display_image(img_path):\n","    img = Image.open(img_path)\n","    plt.imshow(img)\n","    plt.show()\n","\n","def run_app(img_path):\n","  if dog_detector(img_path):\n","        print('Hey dog is detected!')\n","        display_image(img_path)\n","        output = predict_breed_transfer(img_path, model_transfer, class_names)\n","        print(f'It belongs to category...  ')\n","        print(f'{output} \\n')\n","\n","  else:\n","        print('Error! nothing detected!.... check your image once!\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UaLSiHPCpf7T"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMSqYsF68vkNb0jzHCBm2OM","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
